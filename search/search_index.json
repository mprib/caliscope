{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Caliscope is a GUI-based and permissively licensed multicamera calibration package that integrates with 2D landmark tracking tools to produce low-cost, open-source motion capture.</p> <p>The package includes sample markerless trackers using variations of Google's Mediapipe (hands/pose/holistic) which illustrate how to implement the underlying Tracker base class. The intention is to allow alternate tracking algorithms to be cleanly plugged into the pipeline.</p> <p>The workflow currently requires you to provide your own synchronized frames or to provide a file that specifies the time at which each frame was read so that caliscope can time-align the frames itself. A companion project is currently in development (multiwebcam) that can perform concurrent USB webcam video for use cases where differences in frame capture of a few hundredths of a second are tolerable.</p> <p>The installation guide will walk you through the process of installing the package on your system. Project Setup will show you the format for saving files so that they can be used. The workflow guides to the left will provide details about how to create a ChArUco board, calibrate the cameras (both intrinsic and extrinsic) and perform 3D landmark tracking from motion capture trials.</p> <p>This project is at a very early stage so please bear with us while going through the inevitable growing pains that are ahead. You feedback is appreciated. If you have specific recommendations, please consider creating an issue. If you have more general questions or thoughts about the project, please open up a thread in the discussions.</p>"},{"location":"calibration_board/","title":"Charuco Board","text":"<p>Camera calibration is performed with the aid of a ChArUco calibration board. This is basically a chessboard that has ArUco markers within it and is used to determine both the intrinsic properties of a camera (optical center, focal length, and lens distortion) as well as where the cameras are relative to each other. </p> <p>It is possible to purchase pre-printed calibration boards and then set the parameters of the ChArUco to fit the board you have. Please keep in mind that row count and column count are not interchangeable, so visually inspect your physical board compared to the board shown in the GUI to make sure they are the same. You may need to swap row and column values.</p>"},{"location":"calibration_board/#charuco-tab-options","title":"ChArUco Tab Options","text":"<p>ChArUco board creation allows the following configuration options:</p>"},{"location":"calibration_board/#1-row-and-column-count","title":"1. Row and Column Count","text":"<p>While more rows and columns mean that you can get more recognized corners per board view, it results in smaller ArUco markers for a given board size. These smaller markers will then be harder to recognize from a distance, limiting the size of the capture volume you can calibrate for a given resolution of cameras. This presents a trade-off to consider for your set up.</p> <p>As noted above: row count and column count are not interchangeable. An 8x5 board is not the same as a 5x8 board.</p>"},{"location":"calibration_board/#2-board-size","title":"2. Board Size","text":"<p>This is the target size that the final printed board will have. This can minimize white space in the <code>png</code>. </p>"},{"location":"calibration_board/#3-board-inversion","title":"3. Board Inversion","text":"<p>To save on printer ink, you can select to invert the board image so that white and black regions are swapped. If this is done, then caliscope will invert a grayscale image prior to running ChArUco recognition so that things will still work out.</p>"},{"location":"calibration_board/#4-save-calibration-board-png","title":"4. Save Calibration Board <code>.png</code>","text":"<p>Only used for printing out. This file does not need to be saved anywhere in particular. </p>"},{"location":"calibration_board/#5-save-mirror-board-png","title":"5. Save Mirror Board <code>.png</code>","text":"<p>In addition to a regular one-sided board, it is possible to print a double-sided board that has the mirror image printed on the back. If the tracker does not find the primary board in a frame, it will then flip the image and search for the board again, flipping the coordinates of any tracked points back to an unflipped frame of reference.</p> <p>This allows cameras that do not share a common view of a board to better estimate their position relative to each other. The intention is to allow better calibration of systems with surround camera setups as is common in larger scale motion capture. Please note that thicker foam boards may not work well for this. The feature has only been tested with a paper print pressed between two panes of glass.</p>"},{"location":"calibration_board/#6-actual-printed-square-size","title":"6. Actual Printed Square Size","text":"<p>To ensure that the scale of the world is accurate in your final triangulated points, measure the actual length of a printed square of the board. While errors in this measurement will not cause failure along the way, it can result in very large or very small subjects in the triangulated output. Please note that even if you print directly to your intended board size, small differences in actual square size are likely to result.</p>"},{"location":"calibration_board/#implementation-details","title":"Implementation Details","text":""},{"location":"calibration_board/#flatness-matters-particularly-for-intrinsics","title":"Flatness Matters Particularly for Intrinsics","text":"<p>Having a truly flat board is crucial for a good intrinsic camera calibration. At the core of the calibration algorithm is the fact that all points exist on a common plane. Loosely taped pieces of paper or warped cardboard backings will undermine the calibration quality. </p>"},{"location":"calibration_board/#different-boards-for-intrinsic-and-extrinsic-calibrations","title":"Different boards for intrinsic and extrinsic calibrations","text":"<p>It is possible to perform the intrinsic calibration and extrinsic calibration using different boards, though you will have to adjust the board definition prior to running each calibration. </p> <p>In this way, a more perfectly flat and and aligned board could be used for intrinsic calibration when it is fine to have the board relatively close to a single camera. During extrinsic calibration, a larger board pieced together from multiple sheets of paper could be used to allow calibration of a larger capture volume.</p> <p>Please note that for a given set of cameras, the intrinsic calibration only needs to be performed once, and that configuration can be copied over to future projects with the same cameras.</p>"},{"location":"data_capture/","title":"Data capture","text":"<p>This is a placeholder for future documentation that is currently on the todo list. Please see the demo video for a sample data capture. </p>"},{"location":"extrinsic_calibration/","title":"Capture Volume: Extrinsic Calibration","text":""},{"location":"extrinsic_calibration/#processing-steps","title":"Processing steps","text":"<ol> <li>Save videos to <code>project_root/calibration/extrinsic/</code> according to the naming convention outlined in Project Setup</li> <li>Ensure that videos were synchronized when recording, or provide a <code>frame_time_history.csv</code> file so that caliscope can perform the synchronization during processing.</li> <li>You may need to reload the workspace for the <code>Calibrate Capture Volume</code> button to become enabled</li> <li>Pressing <code>Calibrate Capture Volume</code> will initiate the calibration. The final log statement at complete will indicate that <code>point_esimates.toml</code> has been saved. at this point you can reload the workspace and the <code>Capture Volume</code> tab will become enabled.</li> <li>On the <code>Capture Volume</code> tab you can visually inspect the relative position of the cameras according to the calibration</li> <li>Set the board origin to a given frame to align the world frame of reference with the board position. This can be refined by flipping the axes.</li> </ol>"},{"location":"extrinsic_calibration/#practical-recording-guidelines","title":"Practical Recording Guidelines","text":"<ol> <li> <p>Ensure Coverage and Overlap:</p> <ul> <li>Cover the entire volume where the cameras' fields of view overlap with the Charuco board movements.</li> <li>Ensure there's sufficient overlap in the fields of view of the different cameras. This overlap is critical for multi-camera calibration.</li> </ul> </li> <li> <p>Use a board with sufficiently large squares</p> <ul> <li>Larger ArUco markers can be identified from farther away allowing a larger capture volume to be calibrated.</li> </ul> </li> <li> <p>Minimize motion blur</p> <ul> <li>motion blur can substantially compromise corner recognition</li> <li>using a higher frame rate can reduce motion blur</li> <li>this will require more light to maintain good illumination</li> </ul> </li> <li> <p>Consistent Focus:</p> <ul> <li>Use manual focus if available to keep the focus consistent throughout the filming. </li> <li>Auto-focus can introduce inconsistencies </li> </ul> </li> <li> <p>Use the board to define the origin</p> <ul> <li>this is for convenience and not a strict requirement </li> <li>touch the board to the ground while it is held vertically</li> <li>ensure that the top left corner of the board (as shown on the <code>Charuco</code> tab) is in view of the camera and touching the ground</li> </ul> </li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#can-i-process-videos-i-pre-recorded-with-gopros-some-phones-etc","title":"Can I process videos I pre-recorded with GoPros, some phones, etc?","text":"<p>Absolutely! Caliscope supports processing of pre-recorded videos, including those from GoPros and smartphones. Keep in mind that frame synchronization is crucial for accurate calibration and triangulation. We're actively working on an automated solution to synchronize frames using audio data. For now, you'll need to provide your own timestamp data in the specified format or ensure that all video sources are synchoronized while the recording is being made.</p>"},{"location":"faq/#how-can-we-use-something-other-than-mediapipe-for-tracking","title":"How can we use something other than Mediapipe for tracking?","text":"<p>We are prioritizing the integration of alternative tracking tools like MMPose and DeepLabCut into our project roadmap. While Mediapipe variations (hand, pose, holistic) are currently implemented for their simple hardware implementation and as examples for the Tracker base class, any Python object that accepts a frame and outputs points can be integrated. By adhering to the Tracker API, you can easily plug into the entire Caliscope workflow.</p>"},{"location":"faq/#can-the-software-export-to-blender-or-unrealmayaetc","title":"Can the software export to Blender (or Unreal/Maya/etc)?","text":"<p>As of now, Caliscope exports 3D estimates in <code>csv</code> and <code>trc</code> formats, with <code>trc</code> being tailored for biomechanics. An early-stage companion project, Rigmarole, aims to develop a Blender plugin that creates a scaled and animated rig from Caliscope's outputs. This plugin is in early development but was instrumental in creating the animated ballet dancer showcased on our main repo page.</p>"},{"location":"faq/#does-it-do-real-time-processing","title":"Does it do real-time processing?","text":"<p>Currently, Caliscope does not support real-time processing. Landmark tracking across multiple camera views is resource-intensive and challenging to execute in real-time on standard hardware. While real-time tracking and triangulation are feasible (as demonstrated in our demos), they come with limitations in resolution and redundancy of views. Enhancing real-time capabilities is not a current roadmap priority.</p>"},{"location":"faq/#what-is-happening-with-my-data-are-you-storing-videos-i-record","title":"What is happening with my data? Are you storing videos I record?","text":"<p>Absolutely not. Caliscope operates entirely locally on your machine. Envisioning future use in clinical or research settings, we prioritize data privacy. Your control over your data is fundamental to our commitment, ensuring all operations are confined to your local environment.</p>"},{"location":"installation/","title":"Installation","text":"<p>We recommend installing Caliscope using <code>uv</code>, which is a modern, high-performance tool that simplifies and accelerates the installation process.</p>"},{"location":"installation/#1-check-for-uv","title":"1. Check for <code>uv</code>","text":"<p>First, check if you already have <code>uv</code> installed by running the following command in your terminal:</p> <pre><code>uv --version\n</code></pre> <p>If a version number is printed (e.g., <code>uv 0.8.5</code>), you can skip to step 3. If you see an error that the command is not found, please proceed to the next step.</p>"},{"location":"installation/#2-install-uv-if-needed","title":"2. Install <code>uv</code> (if needed)","text":"<p>If you don't have <code>uv</code> installed, you can install it with a single command.</p> WindowsmacOS &amp; Linux <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"installation/#3-create-a-virtual-environment","title":"3. Create a virtual environment","text":"<p>We strongly advise installing Caliscope within a virtual environment to avoid conflicts with other packages. Caliscope is compatible with Python 3.10 and 3.11.</p> WindowsmacOSLinux <pre><code># Navigate to the directory that will hold your project\ncd path\\to\\your\\project\n\n# Create a virtual environment named '.venv' using Python 3.10\nuv venv --python 3.11\n\n# Activate the virtual environment\n.\\.venv\\Scripts\\activate\n</code></pre> <pre><code># Note that there are some environment variables that must be set on MacOS\n# to ensure everything works:\nexport MKL_NUM_THREADS=1\nexport NUMEXPR_NUM_THREADS=1\nexport OMP_NUM_THREADS=1\n\n# Navigate to the directory that will hold your project\ncd path/to/your/project\n\n# Create a virtual environment named '.venv' using Python 3.10\nuv venv --python 3.11\n\n# Activate the virtual environment\nsource .venv/bin/activate\n</code></pre> <pre><code># Install prerequisite packages for GUI display (Ubuntu)\nsudo apt-get update\nsudo apt-get install --fix-missing libgl1-mesa-dev\n\n# Navigate to the directory that will hold your project\ncd path/to/your/project\n\n# Create a virtual environment named '.venv' using Python 3.10\nuv venv --python 3.11\n\n# Activate the virtual environment\nsource .venv/bin/activate\n</code></pre>"},{"location":"installation/#4-install-caliscope","title":"4. Install Caliscope","text":"<p>With your virtual environment activated, you can now install Caliscope using <code>uv</code>.</p> <pre><code>uv pip install caliscope\n</code></pre> <p>Installation may take a moment as some dependencies are large, but <code>uv</code>'s performance makes this process significantly faster than traditional tools.</p>"},{"location":"installation/#5-launch-from-the-command-line","title":"5. Launch from the command line","text":"<p>With the package installed and the virtual environment activated, the main GUI can be launched by running:</p> <pre><code>caliscope\n</code></pre> <p>Note on First Launch: The first time you launch after installation, you might experience a longer startup time. This is normal as the application performs initial setup tasks. Subsequent launches will be significantly faster.</p>"},{"location":"intrinsic_calibration/","title":"Cameras: Intrinsic Calibration","text":""},{"location":"intrinsic_calibration/#processing-steps","title":"Processing steps","text":"<ol> <li>Save calibration video to <code>project_root/calibration/intrinsic/</code> with the filename in the format of <code>port_#.mp4</code> as described in Project Setup</li> <li>Reload the workspace if needed so that the<code>Camera</code> tab becomes enabled </li> <li>On the specific Camera sub-tab, ensure that the video is loaded correctly</li> <li>Confirm by scrolling through the video that the calibration board corners are being recognized (red dots placed on them)</li> <li>(Option 1) Manual Board Selection</li> <li>Scroll through the calibration footage and select <code>Add Grid</code> to include the frame in your calibration data. Grid images should accumulate for all grids included in the intrinisc calibration. </li> <li>When you have chosen the frames you like, click <code>Calibrate</code> to begin the calibration process. </li> <li>(Option 2) Autocalibrate</li> <li>Select the target number of boards for your calibration (~20 works well) </li> <li>Select the percent of the board that must be identified for it to be included in the calibration data (the \"Board Threshold\")</li> <li>click <code>Autocalibrate</code></li> <li>The video will play and calibration data will be periodically stored. At the conclusion of the video the calibration will be performed and the updated camera parameters will be displayed in the GUI (and stored in the <code>config.toml</code> file at the project root).</li> </ol> <p>NOTE: Intrinsic calibration only needs to be performed once per camera. Previously determined values can be carried over to a new project's <code>config.toml</code> file when using the same cameras in a new setup. </p>"},{"location":"intrinsic_calibration/#multiwebcam","title":"MultiWebCam","text":"<p>While conventionally synchronized video data with specialized cama </p>"},{"location":"intrinsic_calibration/#practical-recording-guidelines","title":"Practical Recording Guidelines","text":"<ol> <li> <p>Feel free to move the camera or the board</p> <ul> <li>it can be easier to collect good data when directly monitoring the view of the camera</li> <li>the camera does not need to be in the same position as it is during the extrinsic calibration (and the calibration board doesn't need to be the same either)</li> </ul> </li> <li> <p>Minimize Motion Blur:</p> <ul> <li>make movements slow and smooth </li> <li>Use a high shutter speed to reduce motion blur. </li> <li>Ensure adequate lighting to allow for a faster shutter speed without underexposing the video.</li> </ul> </li> <li> <p>Provide Foreshortening:</p> <ul> <li>Hold the calibration board at various angles relative to the camera. This introduces foreshortening, which is crucial for the calibration process as it provides more information about the camera\u2019s lens characteristics.</li> <li>Include a mix of positions: some shots with the board tilted towards the camera, some away, and others at an angle.</li> </ul> </li> <li> <p>Cover the Entire Field of View:</p> <ul> <li>Move the calibration board throughout the entire field of view of the camera. This ensures that the calibration accounts for lens distortions and other characteristics across the whole image sensor.</li> </ul> </li> <li> <p>Use a High-Quality Calibration Board:</p> <ul> <li>The board should be printed on a flat, rigid material to prevent warping.</li> </ul> </li> <li> <p>Vary the Distance:</p> <ul> <li>Film the calibration board at different distances from the camera. This variation helps in understanding how the camera focuses at different depths.</li> </ul> </li> <li> <p>Consistent Focus:</p> <ul> <li>Use manual focus if available to keep the focus consistent throughout the filming. </li> <li>Auto-focus can introduce inconsistencies as it may change between shots.</li> </ul> </li> <li> <p>Adequate Lighting:</p> <ul> <li>Ensure the scene is well-lit to avoid noise and grain in the video, which can interfere with the calibration process.</li> <li>Avoid strong direct light sources that can cause glare or shadows on the calibration board.</li> </ul> </li> </ol>"},{"location":"mkdocs_notes/","title":"Reminders for using MkDocs","text":"<p>This will likely get deleted soon, but I just want a place to store reminders of the workflow here...</p> <p>For full documentation visit mkdocs.org. test</p>"},{"location":"mkdocs_notes/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs_notes/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"post_processing/","title":"Post-Processing: Landmark Triangulation from Motion Capture","text":"<p>demonstration video coming soon...</p>"},{"location":"post_processing/#processing-steps","title":"Processing steps","text":"<ol> <li>Save videos to dedicated subfolders within <code>project_root/recordings/</code> according to the naming convention outlined in Project Setup</li> <li>Ensure that videos were synchronized when recording, or provide a <code>frame_time_history.csv</code> file so that caliscope can perform the synchronization during processing.</li> <li>You may need to reload the workspace for the recordings to appear in the <code>PostProcessing</code> tab</li> <li>Select which tracker you would like to apply</li> <li>Click the <code>Process</code> button to begin the landmark tracking and triangulation.</li> <li>3D landmark positions will be visualized and you can open the subfolder to inspect the landmark tracking on the recordings or to access the trajectory output files</li> </ol>"},{"location":"post_processing/#tracker-outputs","title":"Tracker Outputs","text":"<p>Current options for the tracker outputs are built on Google's Mediapipe and include pipelines for general Pose, Hands, and Face. The Holistic tracker combines all three outputs. While the Holistic tracker offers improved tracking of the face and hands compared to the Pose model, the number of points it supplies can quickly become unweildy (several hundred for the face). The Simple Holistic model filters out many of these points that may be extraneous to users primarily interested in gross skeletal movement. </p>"},{"location":"post_processing/#metarig-generation","title":"Metarig Generation","text":"<p>For the Simple Holistic tracker you can generate a metarig configuration file. This will provide a set of parameters that can scale segments of a skeletal model based on the average distances between various landmarks throughout a dynamic calibration motion trial where the subject flexes and extends their joints with minimal camera occlusion.</p> <p>With a more accurately scaled skeletal model, inverse kinematics can more successfully approximate the true movement.</p>"},{"location":"post_processing/#practical-recording-guidelines","title":"Practical Recording Guidelines","text":"<ol> <li>Minimize motion blur<ul> <li>motion blur can substantially compromise landmark recognition</li> <li>using a higher frame rate can reduce motion blur</li> <li>this will require more light to maintain good illumination</li> </ul> </li> </ol>"},{"location":"project_setup/","title":"Workspace Setup","text":""},{"location":"project_setup/#initial-project-structure","title":"Initial Project Structure","text":"<p>When a new project is created, the workspace will automatically populate the necessary folder structure if it does not already exist. There are 2 primary folders: <code>calibration</code> and <code>recordings</code>. Within <code>calibration</code> there must be subfolders for <code>intrinsic</code> and <code>extrinsic</code>. All motion capture trials must be stored separately within subfolders of <code>recordings</code> by the user.</p> <p>An configuration file called <code>config.toml</code> will be automatically created when a new project is created. Initially this will only be storing the default charuco board definition. </p> <p>An example initial project folder structure would therefore look like this: <pre><code>ProjectDirectory/\n\u251c\u2500\u2500 config.toml    # Only contains default charuco board definition\n\u251c\u2500\u2500 calibration/\n\u2502   \u251c\u2500\u2500 intrinsic/\n\u2502   \u2514\u2500\u2500 extrinsic/\n\u2514\u2500\u2500 recordings/    # Empty by default prior to user populating data\n</code></pre></p>"},{"location":"project_setup/#stage-1-intrinsic-calibration","title":"Stage 1: Intrinsic Calibration","text":"<p>Place video files for the intrinsic camera calibration in the <code>intrinsic</code> folder. </p> <p>These must follow the naming convention <code>port_1.mp4</code>, <code>port_2.mp4</code>, etc. They do not need to be synchronized. </p> <p>A project with 3 cameras would therefore look something like this going into the intrinsic camera calibration. </p> <pre><code>ProjectDirectory/\n\u251c\u2500\u2500 config.toml          # following intrinsic calibration, this file will also have the camera matrix and distortion for each source camera\n\u251c\u2500\u2500 calibration/\n\u2502   \u251c\u2500\u2500 intrinsic/\n\u2502   \u2502   \u251c\u2500\u2500 port_1.mp4   # These files do not need to be synchronized\n\u2502   \u2502   \u251c\u2500\u2500 port_2.mp4   # Unsynchronized files\n\u2502   \u2502   \u2514\u2500\u2500 port_3.mp4   # Unsynchronized files\n\u2502   \u2514\u2500\u2500 extrinsic/\n\u2514\u2500\u2500 recordings/\n</code></pre> <p>As the intrinsic properties of the camera are calculated, parameters are stored in <code>config.toml</code> at the project root.</p>"},{"location":"project_setup/#stage-2-extrinsic-calibration","title":"Stage 2: Extrinsic Calibration","text":"<p>Place sychronized video files in the <code>extrinsic</code> folder. Synchronization can be accomplished in one of two ways:</p> <ol> <li> <p>Record all video footage with a common external trigger such that each frame is at the same point in time as the corresponding frames from the other files. In other words: all mp4 files should start and stop at the same moment in time and have the same number of frames.</p> </li> <li> <p>Provide a file called <code>frame_time_history.csv</code> within the folder of recorded video. This must provide the time at which each frame was read so that caliscope can synchronize the footage during processing. A companion project, MultiWebCam, creates this file automatically while it manages concurrent recording from multiple webcams and was the source of the Sample Project data found in the docs</p> </li> </ol>"},{"location":"project_setup/#frame_time_historycsv","title":"<code>frame_time_history.csv</code>","text":"<p>This file will have a structure like this:</p> <pre><code>port,frame_time\n3,927387.33536115\n4,927387.50128975\n1,927387.3530109001\n3,927387.50643105\n1,927387.51819965\n2,927387.5063038999\n3,927387.6684489499\n4,927387.66848565\n3,927387.8359558999\n4,927387.8360615501\n...\n</code></pre> <p>It does not need to be in any special order. The numbers shown above are from <code>time.perf_counter()</code> in the standard python library, but any numerical value that shows the relative time of the frame reads will work. There do not need to be the same number of frames within each <code>mp4</code> file. They do not need to start on the same frame. The synchronization will take place automatically, including inserting a blank frame when necessary to keep the video streams aligned in time.</p>"},{"location":"project_setup/#final-file-structure-following-extrinsic-calibration","title":"Final file structure following extrinsic calibration","text":"<p>Following the extrinsic calibration, an additional file called <code>point_estimates.toml</code> will be created. This contains data used to estimate the relative camera translations and rotations. A project with fully calibrated extrinsics would thus look something like this:</p> <pre><code>ProjectDirectory/\n\u251c\u2500\u2500 config.toml          # Now contains rotation and translation parameters for each camera in addition to the distortion and camera matrix\n\u251c\u2500\u2500 point_estimates.toml # Contains charuco data used to estimate the relative camera positions\n\u251c\u2500\u2500 calibration/\n\u2502   \u251c\u2500\u2500 intrinsic/       # directory unchanged from above\n\u2502   \u2502   \u251c\u2500\u2500 port_1.mp4   \n\u2502   \u2502   \u251c\u2500\u2500 port_2.mp4   \n\u2502   \u2502   \u2514\u2500\u2500 port_3.mp4   \n\u2502   \u2514\u2500\u2500 extrinsic/\n\u2502       \u251c\u2500\u2500 frame_time_history.csv  # Time reference for frame synchronization (optional)\n\u2502       \u251c\u2500\u2500 port_1.mp4              # Must be synchronized or use frame_time_history.csv\n\u2502       \u251c\u2500\u2500 port_2.mp4              # Must be synchronized or use frame_time_history.csv\n\u2502       \u2514\u2500\u2500 port_3.mp4              # Must be synchronized or use frame_time_history.csv\n\u2514\u2500\u2500 recordings/\n</code></pre>"},{"location":"project_setup/#stage-3-processing-motion-capture-trial","title":"Stage 3: Processing Motion Capture Trial","text":"<p>For each motion capture trial, create a subfolder within <code>recordings</code> and populate it with synchronized footage as was done with the extrinsic calibration. After post-processing of the video footage has occurred, output will be created as shown in the following example:</p> <pre><code>ProjectDirectory/\n\u251c\u2500\u2500 config.toml                             # File unchanged from above\n\u251c\u2500\u2500 point_estimates.toml                    # File unchanced from above\n\u251c\u2500\u2500 calibration/                            # Entire calibration directory unchanged from above\n\u2502   \u251c\u2500\u2500 intrinsic/\n\u2502   \u2502   \u251c\u2500\u2500 port_1.mp4   \n\u2502   \u2502   \u251c\u2500\u2500 port_2.mp4   \n\u2502   \u2502   \u2514\u2500\u2500 port_3.mp4   \n\u2502   \u2514\u2500\u2500 extrinsic/\n\u2502       \u251c\u2500\u2500 frame_time_history.csv  \n\u2502       \u251c\u2500\u2500 port_1.mp4              \n\u2502       \u251c\u2500\u2500 port_2.mp4              \n\u2502       \u2514\u2500\u2500 port_3.mp4              \n\u2514\u2500\u2500 recordings/\n    \u2514\u2500\u2500 recording_1/                              # can be named anything; contents follow formatting of extrinsic calibration folder\n        \u251c\u2500\u2500 frame_time_history.csv                # optional file; not needed if all video synchronized frame-for-frame\n        \u251c\u2500\u2500 port_1.mp4                      \n        \u251c\u2500\u2500 port_2.mp4                      \n        \u251c\u2500\u2500 port_3.mp4                      \n        \u2514\u2500\u2500 HOLISTIC/                             # Output subfolder created when running Holistic Mediapipe Tracker \n            \u251c\u2500\u2500 frame_time_history.csv            # Matches file in parent folder\n            \u251c\u2500\u2500 port_0_HOLISTIC.mp4               # Copy of file in parent folder with visualized landmarks\n            \u251c\u2500\u2500 port_1_HOLISTIC.mp4               # Copy of file in parent folder with visualized landmarks\n            \u251c\u2500\u2500 port_2_HOLISTIC.mp4               # Copy of file in parent folder with visualized landmarks\n            \u251c\u2500\u2500 xy_HOLISTIC.csv                   # All 2D tracked points by source and point id\n            \u251c\u2500\u2500 xyz_HOLISTIC.csv                  # Triangulated output by point ID\n            \u251c\u2500\u2500 xyz_HOLISTIC_labelled.csv         # Triangulated output in tidy format with labelled x, y, z columns\n            \u2514\u2500\u2500 xyz_HOLISTIC.trc                  # Triangulated Landmark data for OpenSim\n</code></pre>"},{"location":"requirements/","title":"Minimum Requirements","text":"<ul> <li> <p>Python version: Python 3.10 or 3.11</p> </li> <li> <p>Calibration Board: A ChArUco board is needed for camera calibration and determining spatial relationships between multiple cameras. A sample board can be printed from the GUI on a standard 8.5 x 11 sheet of paper. For a given resolution of cameras, a larger board will be able to calibrate a larger capture volume because it can be recognized from farther away. It is crucial to place the printed board on a flat surface to ensure accurate calibration, such as taping it down to a rigid flat piece of cardboard.</p> </li> <li> <p>Capture Environment: Data recording requires a well-lit and evenly lit environment. It is beneficial if the background contrasts highly with the individual being tracked. For example, tracking difficulties may arise if a person in dark clothing stands against a similarly dark wall. Be mindful of clothing, background, and lighting to optimize the quality of the captured data.</p> </li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<ul> <li> synchronized webcam recording  (near term goal..next month)</li> <li> <p>while the scale of webcam based workflows is limited by the I/O bottlenecks of a single computer, it can provide a simplified way to record motion capture in a smaller capture volume where high fps recording is not imperitive</p> </li> <li> <p> MMPose tracker integration  (mid term goal....next 6 months)</p> </li> <li> <p> DeepLabCut integration (mid term goal...next 6 months)</p> </li> <li> <p> synchronized single board computer camera platform (longer term goal.... &gt; 6 months)</p> </li> <li>using Raspberry Pi or similar with MIPI style camera, record high resolution/high frame rate synchronized frames</li> </ul>"},{"location":"sample_project/","title":"Sample Project","text":"<p>A complete walkthrough of the calibration and post-processing workflow is shown below. To determine how <code>caliscope</code> runs on your hardware, you can download the sample dataset used in this video here.</p> <p>Please note that this demo project is meant to illustrate the workflow as simply as possible.There are a variety of improvements that could be made to improve the final results, including:</p> <ul> <li>more cameras</li> <li>higher resolution and frame rate</li> <li>better lighting</li> <li>larger calibration board</li> <li>hardware synchronization rather than folding together frames based on timestamps</li> </ul>"},{"location":"triangulation_api/","title":"Triangulation API","text":"<p>Beyond the GUI interface, Caliscope provides a Python API for programmatically triangulating 2D points based on a calibrated camera array. This is intended for batch processing, custom workflows, or integration with other tools.</p>"},{"location":"triangulation_api/#basic-usage","title":"Basic Usage","text":"<p>The core function for triangulation is <code>triangulate_from_files()</code>, which takes paths to your configuration file and 2D point data, then returns triangulated 3D points. With an optional argument these can be saved to a <code>csv</code>.</p> <pre><code>from pathlib import Path\nfrom caliscope.triangulate.triangulation import triangulate_from_files\n\n# Define paths\nconfig_path = Path(\"/path/to/project/config.toml\")\nxy_path = Path(\"/path/to/project/xy_data.csv\")\noutput_path = Path(\"/path/to/project/xyz_data.csv\")\n\n# Perform triangulation\nxyz_data = triangulate_from_files(\n    config_path=config_path,\n    xy_path=xy_path, \n    output_path=output_path\n)\n\nprint(f\"Triangulated {len(xyz_data)} points\")\n</code></pre>"},{"location":"triangulation_api/#parameter-details","title":"Parameter Details","text":""},{"location":"triangulation_api/#inputs","title":"Inputs","text":"<p><code>config_path</code> : Path to <code>config.toml</code> containing camera calibration parameters. You can find this file in the root of the project workspace. </p> <p><code>xy_path</code> : Path to CSV file with 2D point data. The CSV must contain these columns:</p> <ul> <li>sync_index: Integer identifying the synchronized frame number across cameras</li> <li>port: String or integer identifying which camera captured the point</li> <li>point_id: Integer identifying which landmark/point is being tracked</li> <li>img_loc_x: Float X-coordinate in the image (in pixels)</li> <li>img_loc_y: Float Y-coordinate in the image (in pixels)</li> </ul> <p>Each row represents a single 2D point observed by a specific camera at a specific time. Multiple cameras observing the same point_id at the same sync_index enables triangulation.</p> <p><code>output_path</code>(optional): Path where triangulated 3D points will be saved as a CSV file.  If not provided, results are returned but not saved to disk. The function will create parent directories if they don't exist.</p>"},{"location":"triangulation_api/#returned-value","title":"Returned Value","text":"<p>Pandas dataframe containing triangulated 3D points with columns:</p> <ul> <li>sync_index: Integer identifying the synchronized frame number</li> <li>point_id: Integer identifying which landmark/point was triangulated</li> <li>x_coord: Float X-coordinate in 3D space </li> <li>y_coord: Float Y-coordinate in 3D space </li> <li>z_coord: Float Z-coordinate in 3D space </li> </ul> <p>Each row represents a single 3D point at a specific time.  The coordinates are in the world coordinate system defined during the calibration process.</p>"},{"location":"triangulation_api/#notes","title":"Notes","text":"<ul> <li>For successful triangulation, each point_id must be visible in at least   two cameras at the same sync_index.</li> <li>Points that cannot be triangulated (due to insufficient camera views)   will not appear in the output DataFrame.</li> <li>The pipeline is fairly \"raw\" and would benefit from 2D gap filling prior to triangulation and subsequent gap filling and smoothing.</li> </ul>"},{"location":"video_capture/","title":"Video Capture","text":"<p>Synchronized video footage for extrinsic calibration as well as motion capture can be obtained from specialized camera systems (such as FLIR).</p> <p>As a low-cost alternative, a companion project called MultiWebCam was developed that can allow concurrent recording of frames from multiple webcams, along with time stamps of when the frames were read from the camera. This allows for the frames to be roughly time-aligned in post processing.</p>"},{"location":"workspace/","title":"Workspace","text":"<p>The workspace tab after intrinsic and extrinsic calibration is complete and the videos in <code>project_root/recordings/test_recording/</code> are ready to be processed.</p> <p>Step one of any project is to set the number of cameras you intend to integrate. With this number set, click <code>Reload Workspace</code> to get an updated summary of any missing files or uncalibrated cameras.</p> <p>As input video files are added and cameras are calibrated, you may need to click <code>Reload Workspace</code> for them to be recognized, or for other tabs (i.e. <code>Cameras</code>, <code>Capture Volume</code> and <code>Post Processing</code>) to become available.</p>"}]}